{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0268c14-372f-43a9-86b9-6b4ce67dbca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\sachi\\AppData\\Local\\Temp\\ipykernel_464\\999800599.py\", line 7, in <module>\n",
      "    torch.manual_seed(1337)\n",
      "  File \"c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\torch\\random.py\", line 46, in manual_seed\n",
      "    return default_generator.manual_seed(seed)\n",
      "c:\\users\\sachi\\onedrive\\documents\\github\\envs\\funchat\\lib\\site-packages\\torch\\random.py:46: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  return default_generator.manual_seed(seed)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2626b672950>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from models import BiagramLanguageModel , Head\n",
    "import config\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "883a5453-4a82-47b3-bac4-037b542599c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "\n",
    "data = torch.tensor(encode(text))\n",
    "n = int(0.9*len(text))\n",
    "data_train = data[:n]\n",
    "data_val = data[n:]\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = data_train if split == \"train\" else data_val\n",
    "    idxs = torch.randint(len(data)-config.block_size, (config.batch_size,))\n",
    "    x = torch.stack([data[i:i+config.block_size] for i in idxs])\n",
    "    y = torch.stack([data[i+1:i+config.block_size+1] for i in idxs])\n",
    "    x , y = x.to(config.device) , y.to(config.device)\n",
    "    return x , y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d865f6d2-7dc1-4f88-918f-2b6ec9dd78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiagramLanguageModel(vocab_size).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c3406fd-76f1-4060-8c7e-f6bbcd035bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816705 M parameters\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b06072b-a985-45dc-b189-a299fbd20cf1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc57979-ea4c-4c11-abbc-cb8032c99f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.5231, val loss 4.5183\n",
      "step 100: train loss 2.3918, val loss 2.4095\n",
      "step 200: train loss 2.1215, val loss 2.1554\n",
      "step 300: train loss 1.9237, val loss 2.0077\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "eval_interval = 100\n",
    "max_iters = 10000\n",
    "eval_iters = 200\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf2871a-fe39-4dfb-8bc9-b9296ff9766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.stack([data[50:80],data[80:110]]).to(config.device)\n",
    "\n",
    "ans = model.generate(temp,500)\n",
    "\n",
    "ans = [decode(x.tolist()) for x in ans]\n",
    "\n",
    "for _ in ans:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f64260-9f40-4b1a-9771-a1c178171b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
